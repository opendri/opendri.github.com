---
layout: guide
navgroup: guide
group: guidechapter
section: I. Theory
title: 1. Introduction
excerpt: "xx"
abstract: "Field Guide"
---
{% include JB/setup %}

# 1.	Introduction: The Need for Disaster Risk Management Data

<!-- ## What problems does this approach address? -->
<!-- introduce the core concepts of risk here, as if we are handing the guide to a person entirely new with GFDRR, DRR, and risk management -->

## Problem of Risk Data
Disasters reveal chains of decisions about risk. When infrastructure fails under strain of an earthquake, journalists may point to the failure of the construction firm to adhere to buildings standards. Or to the failure of a government to set and enforce code around retrofitting a school to seismic risks. Or to the owner of a factory to have inquired into the exposure of the structure to hazards and developed a strategy to cope with these potential vulnerabilities. 

In each case, the critical element is missing information. Information that might have driven a different choice about architectural designs, building materials, or the site for the building (siting). Information that might have driven a community to question choices. Information that might have driven a legislature to pass codes or officials to allocate staff time to enforcing them. 

( note: should we add positive deviants into this narrative? )

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
<h4>Natural Hazards, UnNatural Disasters</h4>
<p>Every disaster is unique, but each exposes actions&mdash;by individuals and governments at different levels&mdash;that, had they been different, would have resulted in fewer deaths and less damage.</p>
</div>

## Information is most useful when paired with an understanding of risk
By itself, information is not enough to create a different future. Information must be feed a curiosity about risk--a mindset which inquires into various probably futures and looks at ways to mediate the worst outcomes. 

## Moving beyond Hazard Modeling
Many governments already model hazards. They have the ability to map the energy that weather, seismic activity, and other hazards will impose on a region's landscape. Yet few have the ability to take this curiousity the next step: to model how this energy will impact the built environment and human systems. 


<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
<h4>Haiti then Chile: A study of contrasts</h4>
<p>Nature is a powerful force, yet natural hazards need not generate disasters. By combining engineering and collective action, nations have built resilience in the face of severe shocks. Chile: 8.8 exerted 500 times the energy of the earthquake in Port-au-Prince, yet killed only 234 people versus over 200K citizens of Haiti. This disproportionate effect is the resilience leaders seek to build for their citizens. </p>
</div>


## Risk Assessment and Communication
Risk assessment requires data about both hazards and the exposure of the built environment to specific hazards to measure vulnerability of infrastructure to specific threats. This process requires curation of three dynamic datasets:

1.	**Hazard Data**: a set of historical models and forecasts for hydro-met, seismic, climate, and volcanic dynamics.
 
2.	**Exposure Data**: a dataset about the built environment specifically collected around the metrics necessary to calculating the vulnerability of a building or infrastructure to a specific natural hazard. These data change with the rate of urbanization and retrofitting. After a disaster, they must be updated to reflect the new built environment. 

3.	**Vulnerability Data**: a calculated dataset that shows the effect that one or more hazards will have on the a dynamic built environment.

When countries have all three datasets under management, it is possible to engage in risk assessment: an examination of the primary risks posed by probable futures and strategic planning to mitigate those risks with retrofitting, building codes, and other forms of mitigation, such as drainage, irrigation, seawalls, and other major infrastructure.

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
<h4>Superstorm Sandy in NYC</h4>
<p>Even NYC had problems with exposure data. Anecdote TBD from FEMA.</p>
</div>

## Constraints on the application of information to risk management
Most countries lack the resources, training, and software to place all three types of data under a management process that allows for the assessment and mediation of risk. In many nations, the information necessary to catalyze this type of risk management thinking is blocked by a range of problems, including:

### Fragmentation of Specialists
Risk requires multidisciplinary analysis, but these experts rarely sit in one organization. Specialization has driven the design of modern bureaucracy towards hierarchies. Great strength for transactions and coordination of workflows, but information raises challenge. Cross-cutting horizontal flows of information needed across specializations to coordinate decisions. Design of hierarchies often forces these flows through gates or up-then-down-and-across structures. 

### Data Fragmentation
Multidisciplinary analysis requires data from across specializations, yet these data are often segmented into silos. May be in proprietary formats, locked under intellectual property licenses that require expensive payments. Some ministries charge other parts of their own governments for use of the data. 

### Data Duplication
While donors may not set out to fund two or more collections of the same data, the result of having closed data is often just that. One ministry does not know what that other has or is currently collecting. This problem becomes more acute when NGOs are involved, and communication across sectors is not as good as it might be. Fusion separate datasets may not be possible, or may be very costly, if the groups are using different standards, software, and practices around its collection and quality assurance.

### Data Access
Data may be available, but it might be limited to use by specific parties. Access is discriminatory.

### Data Staleness and Incompleteness
Data may reflect best knowledge from investment made more than decade before. In some countries, the last census or survey-quality map may be decades old. Data about exposure may have never been collected at the level of resolution necessary to build risk models. Data is outdated and/or incomplete.

### Exposure Data can be Expensive to collect and maintain
Data about buildings and built environment is time-intensive to collect and maintain. The built environment changes at the rate of construction plus the rate of destruction. (Collective action to collect this data has become possible.)

## Policy Challenge: Risk information is accelerating
Climate Change and urbanization are changing the nature and magnitude of risks many developing countries, particularly those most at risk of increased cyclones, floods, and droughts in cities with swelling peri-urban slums sited in the most vulnerable areas. For these policy makers, it is become ever more difficult to get a handle on dynamic risks.

### That said, even dynamic risk can be mediated and managed. 
The levers to manage risk are not always obvious. They may be at level of policy, such as building codes. They may require direct investments in retrofitting infrastructure to higher seismic standards. Or they may require soft infrastructure in a better prepared populace who stockpile supplies because they expect to be cut off from outside aid for a period of several weeks after the next major disaster.

### But the components of risk assessment are no longer activities that most governments can afford to do alone; they is an increasing need for collective action.
Governments need collective assistance in building understanding of risks. They need to provide the infrastructure for private sector, public sector, and community organizations to work from a shared understanding of their probable futures, and then to guide the conversation about how to invest in mitigating the worst possible outcomes as a whole nation.

### This need for collective action is emerging at time when communications tools and practices are introducing disruptive change in the methods of coordinating collective action. 
<ul class="circle">
<li>GPS: </li>
<li>Cell Phones:</li>
<li>Overhead imagery: </li>
<li>Participatory or community mapping:</li>
<li>Open geospatial platforms:</li>
</ul>

Resilience can increased through relationships. Through information that activates and coordinates those relationships. Collective action.

But where to put the lever? Requires information on the hazards, exposure of the built environment to those hazards, measured as vulnerability.

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
<h4>Case Study: Kathmandu</h4>
<p>Kathmandu Pre-OpenDRI: the need for basic map of schools and health facilities. Why we could not model seismic risks to critical infrastructure except at the macro level.</p>
</div>

## Recommendation Option: Combine Open Government Data with Collective Action from Community Mapping
Combining top-down and bottom-up. Release government information that allows all actors to coordinate their assessments and mitigation activities around risks, and allow communities to curate the map of their own buildings' exposure to threats from natural hazards. 

***

## Why Open Government Data?
Concept: empower decision makers with open systems. Create shared space where community can assembled around shared problems and develop open solutions. 

## Principles
Operating principles of open data.

1.	Legally versus Technically Open
2.	Accessible: available at a public address (URI)
3.	Interoperable: open standard
4.	Reusable: can be redistributed and reused in ways that were not necessarily anticipated by the curator of the original data.

Overall, the intent is to follow Huffman’s First Principle of Information Sharing: 
Leave data better than you found it.

## Challenges
Open data is not easy to implement. There are many reasons why 
1.	Fragmentation
2.	Incompleteness
3.	Inaccuracy
4.	Transparency: politics of opening data creates visibility into processes that have not previously been transparent. Such exposure creates anxieties.
5.	Trust 

## Value of Open Data
Yet, open data creates new opportunities
1.	Fusion of previously fragmented data
2.	Greater coverage and comprehensive views
3.	Community curation to correct data
4.	Trust in transparent processes
5.	Inexpensive and reusable

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
	<h4>Ten Principles of Open Government Data (OGD)</h4>
	<p><em>(src: Linked Open Data: The Essentials, Bauer and Kaltenböck)</em></p>
	<ol>
	<li>Data must be complete</li>
	<li>Data must be primary</li>
	<li>Data must be timely</li>
	<li>Data must be accessible</li>
	<li>Data must be machine-processable</li>
	<li>Access must be non-discriminatory</li>
	<li>Data formats must be non-proprietary</li>
	<li>Data must be license free</li>
	<li>Data must have permanence, be findable over time</li>
	<li>Usage costs must be <em>de minimus</em></li>
	</ol>
	<p>From the Sebastopol meeting on Open Government Data</p>
</div>

***

## Participatory Mapping: building better exposure data
What is participatory mapping?

### OpenStreetMap

### Community-engagement with mapping

### Data Quality
(see this [discussion of the accuracy and reliability of volunteered geographic information](osm.html))

### Data Curation

### Case: Indonesia





