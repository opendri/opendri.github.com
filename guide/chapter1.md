---
layout: guide
navgroup: guide
group: guidechapter
section: I. Theory
title: 1. Introduction
excerpt: "xx"
abstract: "Field Guide"
---
{% include JB/setup %}
<!-- warning/disclaimer -->
<div class="message-box short error">
<p><strong>NOTE</strong>: This document is a very early sketch by a consultant/writer, placed online for collaborative writing. In no way should this document be viewed as a reflection of the overall team's sense of OpenDRI processes. DO NOT USE for operations.</p>
</div>


# 1.	Introduction: The Need for Disaster Risk Management Data

<!-- ## What problems does this approach address? -->
<!-- introduce the core concepts of risk here, as if we are handing the guide to a person entirely new with GFDRR, DRR, and risk management -->

## Problem of Risk Data
Disasters reveal chains of decisions about risk (NHUD, 2011). When infrastructure fails under strain of an earthquake, journalists may point to the failure of the construction firm to adhere to building standards. Or to the failure of a government to set and enforce code around retrofitting a school to seismic risks. Or to the owner of a factory to have inquired into the exposure of the structure to hazards and developed a strategy to cope with these potential vulnerabilities. 

<!-- ~~~ ~~~ ~~~ SIDEBAR  ~~~ ~~~ ~~~ -->
<div class="info-box image-right adapted width-250px">
<h4>Natural Hazards, UnNatural Disasters</h4>
<p>Every disaster is unique, but each exposes actions&mdash;by individuals and governments at different levels&mdash;that, had they been different, would have resulted in fewer deaths and less damage.</p>
</div>
<!--  ~~~ ~~~ ~~~ END SIDEBAR  ~~~ ~~~ ~~~ -->

In each case, the critical element is missing information. Information that might have driven a different choice about architectural designs, building materials, or the site for the building (siting). Information that might have driven a community to question choices. Information that might have driven a legislature to pass codes or officials to allocate staff time to enforcing them. 

The opposite also holds true. When information about the risk *have* been available, the outcomes have been very different. From Concepcion (Chile, 2011) to Christ Church (NZ, 2012), loss of life has been minimal in comparison to the magnitude of the hazard. The availability of information has opened a valve for investment to flow into resilient infrastructure that functioned as a buffer on the loss of human life.  Although the earthquake in Concepcion was 500 times the energy of the one in Port-au-Prince a month earlier, less than 500 Chileans lost their lives, versus over 120,000 Haitians. Chile's infrastructure had made the area about 120,000-150,000 times more resilient than Haiti: 500 times the energy yet only between 400-500 fatalities. (note: compare to built environment? Chile had more built enviroment damage, with 350,000 buildings, versus Haiti, with 250,000 buildings). This buffer was one part of the resilience of the society. (the other being the ability to bounce back in reconstruction)

***

## Resilience

OpenDRI works on a definition of resilience where informed mitigation prior to a disaster provides a buffer against the loss of human life and property when a hazard occurs. Information about the environment and likely places of failure also can improve response operations and target efforts at saving lives. OpenDRI also engages donors who fund recovery and reconstruction so that nations can build back better than before. 

Before, during, and after a disaster, information exposes decision makers to outcomes that would otherwise remain invisible.

***

## Information is most useful when paired with an understanding of risk
But, information is not by itself enough to create a different future. Information must be feed a curiosity about risk--a mindset which inquires into various probably futures and looks at ways to mediate the worst outcomes.

This curiousity is not always a part of governments in either the developed or developing world. Often, officials want a step-by-step cookbook for what they need to do and clear metrics by which they will be evaluated. Risk requires a different mindset: a desire to peel back layers and explore what would otherwise remain invisible. As a result, collating, collecting, and analyzing data about risk requires a curious approach. 
 
***

## Moving beyond Hazard Modeling
Many governments already model hazards. Ministries have the ability to map the energy that weather, seismic activity, and other hazards will impose on a region's landscape. (research with Lana).

Questions: 
1. What is a typical hazard modeling situation that we encounter prior to an OpenDRI engagement?
2. What plans does Track III have for hydromet and other hazards models?

***

## Risk Assessment and Communication
While governments may be able to model hazards, few have the ability to take this curiousity the next step: to model how this energy will impact the built environment and human systems. 

Risk assessment requires data about both hazards and the exposure of the built environment to specific hazards to measure vulnerability of infrastructure to specific threats. This process requires curation of three dynamic datasets:

<!-- ~~~ ~~~ ~~~ SIDEBAR  ~~~ ~~~ ~~~ -->
<div class="info-box image-right adapted width-250px">
<h4>Haiti then Chile: A study of contrasts</h4>
<p>Nature is a powerful force, yet natural hazards need not generate disasters. By combining engineering and collective action, nations have built resilience in the face of severe shocks. Chile: 8.8 exerted 500 times the energy of the earthquake in Port-au-Prince, yet killed only 234 people versus over 200K citizens of Haiti. This disproportionate effect is the resilience leaders seek to build for their citizens. </p>
</div>
<!-- ~~~ ~~~ ~~~ END SIDEBAR  ~~~ ~~~ ~~~ -->

1.	**Hazard Data**: a set of historical models and forecasts for hydro-met, seismic, climate, and volcanic dynamics.
 
2.	**Exposure Data**: a dataset about the built environment specifically collected around the metrics necessary to calculating the vulnerability of a building or infrastructure to a specific natural hazard. These data change with the rate of urbanization and retrofitting. After a disaster, they must be updated to reflect the new built environment. 

3.	**Vulnerability Data**: a calculated dataset that shows the effect that one or more hazards will have on the a dynamic built environment.

When countries have all three datasets under management, it is possible to engage in risk assessment: an examination of the primary risks posed by probable futures and strategic planning to mitigate those risks with retrofitting, building codes, and other forms of mitigation, such as drainage, irrigation, seawalls, and other major infrastructure.

<!-- ~~~ ~~~ ~~~ SIDEBAR  ~~~ ~~~ ~~~ -->
<div class="info-box image-right adapted width-250px">
<h4>InaSAFE</h4>
<p>Description of the InaSAFE program in Indonesia, how it is used for impact modeling in Jakarta.</p>
</div>
<!-- ~~~ ~~~ ~~~ END SIDEBAR  ~~~ ~~~ ~~~ -->

(note: build info graphic on how these models come together to form a triad that leads to risk assessment, and how higher resolutions improve accuracy of the models. )

![alt text](/assets/images/risk_hazard_exp_vul.png "Risk Assessment Triad: Hazards, Exposure, Vulnerability")

***

## Constraints on the application of information to risk management
Most countries lack the resources, training, and software to place all three types of data under a management process that allows for the assessment and mediation of risk. In many nations, the information necessary to catalyze this type of risk management thinking is blocked by a range of problems, including:

### Fragmentation of Specialists
Risk requires multidisciplinary analysis, but these experts rarely sit in one organization. Specialization has driven the design of modern bureaucracy towards hierarchies. Great strength for transactions and coordination of workflows, but information raises challenge. Cross-cutting horizontal flows of information needed across specializations to coordinate decisions. Design of hierarchies often forces these flows through gates or up-then-down-and-across structures. 

### Data Fragmentation
Multidisciplinary analysis requires data from across specializations, yet these data are often segmented into silos. May be in proprietary formats, locked under intellectual property licenses that require expensive payments. Some ministries charge other parts of their own governments for use of the data. 

<!-- ~~~ ~~~ ~~~ SIDEBAR  ~~~ ~~~ ~~~ -->
<div class="info-box image-right adapted width-250px">
<h4>Superstorm Sandy in NYC</h4>
<p>Even NYC had problems with exposure data. Anecdote TBD from FEMA.</p>
</div>
<!-- ~~~ ~~~ ~~~ END SIDEBAR  ~~~ ~~~ ~~~ -->

### Data Duplication
While donors may not set out to fund two or more collections of the same data, the result of having closed data is often just that. One ministry does not know what that other has or is currently collecting. This problem becomes more acute when NGOs are involved, and communication across sectors is not as good as it might be. Fusion separate datasets may not be possible, or may be very costly, if the groups are using different standards, software, and practices around its collection and quality assurance.

### Data Access
Data may be available, but it might be limited to use by specific parties. Access is discriminatory.

### Data Staleness and Incompleteness
Data may reflect best knowledge from investment made more than decade before. In some countries, the last census or survey-quality map may be decades old. Data about exposure may have never been collected at the level of resolution necessary to build risk models. Data is outdated and/or incomplete.

### Exposure Data can be Expensive to collect and maintain
Data about buildings and built environment is time-intensive to collect and maintain. The built environment changes at the rate of construction plus the rate of destruction. (Collective action to collect this data has become possible.)

***

## Policy Challenge: Risk information is accelerating
Climate Change and urbanization are changing the nature and magnitude of risks many developing countries, particularly those most at risk of increased cyclones, floods, and droughts in cities with swelling peri-urban slums sited in the most vulnerable areas. For these policy makers, it is become ever more difficult to get a handle on dynamic risks.

<!-- ~~~ ~~~ ~~~ SIDEBAR  ~~~ ~~~ ~~~ -->
<div class="info-box image-right adapted width-250px">
<h4>Case Study: Kathmandu</h4>
<p>Kathmandu Pre-OpenDRI: the need for basic map of schools and health facilities. Why we could not model seismic risks to critical infrastructure except at the macro level. Urbanization and growth rates (about 5%!). Rapid construction, low ability to enforce building codes. Ever more population at risk, more structures enter into risk pool. </p>
</div>
<!-- ~~~ ~~~ ~~~ END SIDEBAR  ~~~ ~~~ ~~~ -->

### That said, even dynamic risk can be mediated and managed. 
The levers to manage risk are not always obvious. They may be at level of policy, such as building codes. They may require direct investments in retrofitting infrastructure to higher seismic standards. Or they may require soft infrastructure in a better prepared populace who stockpile supplies because they expect to be cut off from outside aid for a period of several weeks after the next major disaster.

### But the components of risk assessment are no longer activities that most governments can afford to do alone; they is an increasing need for collective action.
Governments need collective assistance in building understanding of risks. They need to provide the infrastructure for private sector, public sector, and community organizations to work from a shared understanding of their probable futures, and then to guide the conversation about how to invest in mitigating the worst possible outcomes as a whole nation.

### This need for collective action is emerging at time when communications tools and practices are introducing disruptive change in the methods of coordinating collective action. 
<ul class="circle">
<li>GPS: </li>
<li>Cell Phones:</li>
<li>Overhead imagery: </li>
<li>Participatory or community mapping:</li>
<li>Open geospatial platforms:</li>
</ul>

Resilience can increased through relationships. Through information that activates and coordinates those relationships. Collective action.

But where to put the lever? Requires information on the hazards, exposure of the built environment to those hazards, measured as vulnerability.

***

## Recommendation Option: Combine Open Government Data with Collective Action from Community Mapping, Hazard Modeling, and Risk Communication

Problem: how to aggregate information about a rapidly changing built environment amid a rapidly changing set of hazards. Has been attempted to be done through traditional governance mechanisms: submission of data on an periodic (often annual or multi-annual) basis. This collection rate is hard to sustain for many governments, but it is not fast enough to deal with the scale of the problem. 

<div class="info-box image-right adapted width-250px">
<p>Coping with the rapid aggregation and analysis of data from hundreds or thousands of potential sources is a problem that society already solved: stock market data. Investing or divesting from any particular form of equity is driven by the history and expected value of a stock price. The value of this price is driven by transaction of informed buyers and sellers. </p>

<p>A market is not the solution to risk data--it is not structured as a price driven by supply and demand. Rather, the mechanism of collective information aggregation is. Allow government to regulate how information flows, who is authorized to access and use it, and freeing some of the information for redistribution and reuse.</p> 

<p>(contact Tom Malone of MIT/CCI: here we have information aggregation mechanism from many sources which retain curation rights over their own data, but the value only derives from aggregation and analysis of the data. Decisions about what actions to take based on the data remain that of individual actors; coordination only generally happens in an emergency or preparation for an emergency. Conflict resolution does not yet exist, beyond maps, which is a wiki model). </p>
</div>
Combining top-down and bottom-up. Release government information that allows all actors to coordinate their assessments and mitigation activities around risks, and allow communities to curate the map of their own buildings' exposure to threats from natural hazards. 

The OpenDRI engagement packages three activities into the work plan:

1. Open Government Risk Data
2. Open Community Mapping Data
3. Open Risk Impact Models (Risk Communication)

(Note: This section needs to be a hard-nosed discussion of why to pursue open data as the best option. Not about ideals, but about the practical nature of open data being the best pathway to cope with the problem of risk assessment in a highly dynamic and less predictable world. This section should be about collective intelligence. )

***

## 1. Open Government Risk Data
Open Government Risk Data (OGRD) programs provide for the strategic release of certain government datasets to the commons, where they can be curated, emended, amended, and (most importantly) reused in ways that governments alone cannot do. OGD is **not** the release of all government data to the public.

The strategic release of data to a public commons enables first recommendation from NHUD: 

<blockquote class="quote-icon">
<p>First, governments can and should make information more easily accessible. People are often guided in their prevention decisions by information on hazards, yet the seemingly simple act of collecting and providing information is sometimes a struggle. While some countries attempt to collect and archive their hazard data, efforts are generally inconsistent or insufficient. Specifically, there are no universal standards for archiving environmental parameters for defining hazards and related data. Data exchange, hazard analysis, and hazard mapping thus become difficult.</p>
</blockquote>

Open data empower decision makers at all levels of government, as well as in the private sector. Open data creates a common space where community can assembled around shared problems and co-develop solutions with a wide range of partners.

### Principles
For data to serve decision makers across a society, it needs to be fully open. This means:

1.	Technically Open: Many government datasets are locked in data formats that can only be read by proprietary software (and sometimes hardware, like obsolete magnetic tape backup drives). The data must be released in ways that allow any device or software can read it.
2. Legally Open: the license under which the data is released must permit redistribution and reuse.
2.	Accessible: the data must be available at a public Internet address (URI)
3.	Interoperable: the data must follow open standards.
4.	Reusable: can be redistributed and reused in ways that were not necessarily anticipated by the curator of the original data.

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
	<h4>Ten Principles of Open Government Data (OGD)</h4>
	<p><em>(src: Linked Open Data: The Essentials, Bauer and Kaltenb√∂ck)</em></p>
	<ol>
	<li>Data must be complete</li>
	<li>Data must be primary</li>
	<li>Data must be timely</li>
	<li>Data must be accessible</li>
	<li>Data must be machine-processable</li>
	<li>Access must be non-discriminatory</li>
	<li>Data formats must be non-proprietary</li>
	<li>Data must be license free</li>
	<li>Data must have permanence, be findable over time</li>
	<li>Usage costs must be <em>de minimus</em></li>
	</ol>
	<p>From the Sebastopol meeting on Open Government Data</p>
</div>

### How OpenDRI works with Governments
OpenDRI advises ministries how on the collation, cleansing, and release of data related to risks. These datasets tend to be spread across governments. Sometimes, ministries sell them to each other (though the revenues tend to be low and the administrative costs for managing these sales tend to be high). OpenDRI partners work together to determine which data are appropriate for release. That said, rather than following the traditional method of aggregating data into a central web portal, OpenDRI recognizes that ministries wish to retain stewardship over their own data. So OpenDRI recommends that each ministry release its data using (free and open source) platforms that allow other ministries to subscribe to the data using web services. This model has a number of benefits:

1. **Politics**: ministries retain control of their own information. Instead of adding a centralized umbrella web portal and the perception of a shift in data ownership, a government adds a free tool into existing workflows. 
2. **Freshness**: the data in the ecosystem is always flowing from the source and is as new as the ministry is able to release.



***

## 2. Open Participatory Mapping Data
Building better exposure data is very time intensive, but it need not be costly. It requires individuals to visit thousands of municipal buildings and locations of critical infrastructure, make a basic assessment about the construction of those sites, take pictures, and ask locals questions about the site. If performed by survey departments of the government or commercial ventures, the costs quickly spiral beyond the means of most governments and donors. In comparison, mapping Kathmandu under OpenDRI cost under $200,000 USD. 

The approach taken by OpenDRI is to recruit and train community members to map their own cities. This method creates jobs for youth, trains them in modern geospatial tools, and prepares them for additional work curating the map of their cities. See [OpenCities](http://opencitiesproject.com/). It also creates a map that is free and open for all to use for any purpose.

<!-- Info Box -->
<div class="info-box image-right adapted width-250px">
	<h4>Peta Gratis Untuk Semua (OpenStreetMap Indonesia)</h4>
	<p>Case study TBD for OSM in Indonesia, from outcast to integrated into BIG.</p>
</div>

### OpenStreetMap
OpenStreetMap aims to create a free and open map of the world. Akin to Wikipedia, it allows anyone to draw on "the map" using a wide range of software and devices, including handheld computers and smartphones. To ensure accuracy and data quality, the OpenStreetMap Foundation works with communities in each country to encourage editors and experienced uses to review submissions, and provides software that makes it relatively easy for experienced users to correct the errors of person who has made a mistake or submitted inaccurate data. It is a community managed map.

While some might expect that the accuracy of the map would therefore be far lower than professional cartography, academic studies show that the map is within the margin of error of consumer GPS devices (see Muki Hakley, University College, London in this [discussion of the accuracy and reliability of volunteered geographic information](osm.html))



***

## 3. Open Risk Impact Models

To change the mindset of planners at all levels of government, it is not only necessary to give them maps and open up government data; they must also have simple tools that allow them to visualize potential futures. Because traditional risk assessment models require a great deal of training and expertise, a range of partners came together to build impact modeling tools that enable a municipal government official to pull hazard data from the national open risk data catalogue, exposure data about her city from OpenStreetMap, and with a few mouse clicks, show the potential impact of a hazard on the schools in her city.

### InaSAFE
Impact models are the brainchild of the partnership between the Government of Indonesia, AIFDR, AusAID, and the World Bank/GFDRR. (history to be continued).


